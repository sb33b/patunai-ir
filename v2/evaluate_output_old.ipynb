{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "227f7fa2",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dac11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a5fcc",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a02c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"outputs/Patunai-IR-Eval.xlsx\"\n",
    "\n",
    "col_facts = \"Premise/Facts\"\n",
    "col_claims = \"Hypothesis/Claims\"\n",
    "col_generated = \"Generated Premise\"\n",
    "\n",
    "entailment_facts_vs_claims = []\n",
    "entailment_gen_vs_claims = []\n",
    "similarity_facts_vs_gen = []\n",
    "\n",
    "df = pd.read_excel(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d635a",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d13401d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n",
      "Using OpenAI embeddings (text-embedding-3-small)\n"
     ]
    }
   ],
   "source": [
    "embedding_model = 'intfloat/e5-base-v2'\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Running on: {self.device}\")\n",
    "\n",
    "        # # 1. NLI - entailment\n",
    "        # print(\"Loading NLI model (cross-encoder/nli-distilroberta-base)...\")\n",
    "        # self.nli_tokenizer = AutoTokenizer.from_pretrained(\"cross-encoder/nli-distilroberta-base\")\n",
    "        # self.nli_model = AutoModelForSequenceClassification.from_pretrained(\"cross-encoder/nli-distilroberta-base\").to(self.device)\n",
    "        # self.entailment_idx = 1\n",
    "        \n",
    "        # 2. Semantic - similarity\n",
    "        print(f\"Loading Semantic model ({embedding_model})...\")\n",
    "        self.sim_model = SentenceTransformer(embedding_model, device=self.device)\n",
    "\n",
    "    # def get_entailment_score(self, premise, hypothesis):\n",
    "    #     if not premise or not hypothesis: \n",
    "    #         return 0.0\n",
    "        \n",
    "    #     inputs = self.nli_tokenizer(\n",
    "    #         premise, hypothesis, return_tensors='pt', truncation=True, max_length=512\n",
    "    #     ).to(self.device)\n",
    "        \n",
    "    #     with torch.no_grad():\n",
    "    #         outputs = self.nli_model(**inputs)\n",
    "    #         probs = torch.softmax(outputs.logits, dim=1)\n",
    "            \n",
    "        # return probs[0][self.entailment_idx].item()\n",
    "\n",
    "    def get_semantic_similarity(self, text1, text2):\n",
    "        if not text1 or not text2: \n",
    "            return 0.0\n",
    "        emb1 = self.sim_model.encode(text1, convert_to_tensor=True)\n",
    "        emb2 = self.sim_model.encode(text2, convert_to_tensor=True)\n",
    "        return util.cos_sim(emb1, emb2).item()\n",
    "\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3fa0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_facts = \"Premise/Facts\"\n",
    "col_claims = \"Hypothesis/Claims\"\n",
    "col_generated = \"Generated Premise\"\n",
    "\n",
    "def safe_text(val):\n",
    "    if pd.isna(val) or val is None:\n",
    "        return \"\"\n",
    "    return str(val).strip()\n",
    "\n",
    "entailment_facts_vs_claims = [] \n",
    "entailment_gen_vs_claims = []   \n",
    "similarity_facts_vs_gen = []    \n",
    "\n",
    "print(f\"Evaluating {len(df)} rows...\")\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    facts = safe_text(row.get(col_facts))\n",
    "    claims = safe_text(row.get(col_claims))\n",
    "    generated = safe_text(row.get(col_generated))\n",
    "    \n",
    "    score_facts_claims = 0.0\n",
    "    score_gen_claims = 0.0\n",
    "    score_sim = 0.0\n",
    "    \n",
    "    # if facts and claims:\n",
    "    #     score_facts_claims = evaluator.get_entailment_score(\n",
    "    #         premise=facts, \n",
    "    #         hypothesis=claims\n",
    "    #     )\n",
    "    # if generated and claims:\n",
    "    #     score_gen_claims = evaluator.get_entailment_score(\n",
    "    #         premise=generated, \n",
    "    #         hypothesis=claims\n",
    "    #     )\n",
    "    if facts and generated:\n",
    "        score_sim = evaluator.get_semantic_similarity(facts, generated)\n",
    "    \n",
    "    # entailment_facts_vs_claims.append(score_facts_claims)\n",
    "    # entailment_gen_vs_claims.append(score_gen_claims)\n",
    "    similarity_facts_vs_gen.append(score_sim)\n",
    "\n",
    "# df['Entailment_Facts_vs_Claims'] = entailment_facts_vs_claims\n",
    "# df['Entailment_Gen_vs_Claims'] = entailment_gen_vs_claims\n",
    "df[embedding_model] = similarity_facts_vs_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbe6b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_analyze = [\n",
    "    # 'Entailment_Facts_vs_Claims',\n",
    "    # 'Entailment_Gen_vs_Claims',\n",
    "    embedding_model\n",
    "]\n",
    "\n",
    "# stats = df[cols_to_analyze].agg(['mean', 'std', 'min', 'max', 'median'])\n",
    "print(\"---Aggregate Statistics---\")\n",
    "# print(stats)\n",
    "print(\"\\n\")\n",
    "\n",
    "# avg_baseline = df['Entailment_Facts_vs_Claims'].mean()\n",
    "# avg_validity = df['Entailment_Gen_vs_Claims'].mean()\n",
    "avg_fidelity = df[embedding_model].mean()\n",
    "print(\"---Summary Report---\")\n",
    "# print(f\"1. Baseline Truth   (Do Facts support Claims?):       {avg_baseline:.2%} confidence\")\n",
    "# print(f\"2. AI Validity      (Does Gen support Claims?):       {avg_validity:.2%} confidence\")\n",
    "print(f\"3. Fidelity         (Is Gen similar to Facts?):       {avg_fidelity:.2%} similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf29ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = os.path.splitext(os.path.basename(path))[0]\n",
    "output_path = f\"outputs/{base_name}_{embedding_model.replace('/','-')}.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "print(f\"Saved results to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patunai-ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
