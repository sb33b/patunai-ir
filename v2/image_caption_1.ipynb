{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSjtDYkkXDXw"
      },
      "source": [
        "# SETUP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvqO31oL_T7c"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_5hjYjdCc-r"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from typing import Optional\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import torch\n",
        "from bs4 import BeautifulSoup\n",
        "from dotenv import load_dotenv\n",
        "from googleapiclient.discovery import build\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.drawing.spreadsheet_drawing import OneCellAnchor, TwoCellAnchor\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyjvDjvIOPtl"
      },
      "outputs": [],
      "source": [
        "# get .env keys\n",
        "load_dotenv()\n",
        "API_KEY = os.getenv(\"API_KEY\")\n",
        "CSE_ID = os.getenv(\"CSE_ID\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUGBqya9_ZsS"
      },
      "source": [
        "## Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ-ckmscXtN0"
      },
      "outputs": [],
      "source": [
        "# check for Ollama instance\n",
        "for i in range(30):\n",
        "    try:\n",
        "        r = requests.get(\"http://localhost:11434/api/tags\", timeout=1)\n",
        "        if r.status_code == 200:\n",
        "            print(\"Ollama served at http://localhost:11434/\")\n",
        "            break\n",
        "    except Exception:\n",
        "        time.sleep(1)\n",
        "else:\n",
        "    raise RuntimeError(\"Ollama failed to start.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GwITGgkkYgx_"
      },
      "outputs": [],
      "source": [
        "# pull models from https://ollama.com/library\n",
        "# ! ollama pull qwen3-embedding:0.6b\n",
        "# ! ollama pull qwen3:0.6b\n",
        "# ! ollama pull qwen2.5vl:3b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwDXmXTgF2op"
      },
      "source": [
        "# CLAIM PROCESSING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "W_rEm6En59Xq"
      },
      "outputs": [],
      "source": [
        "# download nltk resources\n",
        "NLTK_DATA_DIR = os.path.join(os.getcwd(), \"cache/nltk_data\")\n",
        "os.makedirs(NLTK_DATA_DIR, exist_ok=True)\n",
        "nltk.data.path.append(NLTK_DATA_DIR)\n",
        "nltk.download(\"punkt\", quiet=True, download_dir=NLTK_DATA_DIR)\n",
        "nltk.download(\"punkt_tab\", quiet=True, download_dir=NLTK_DATA_DIR)\n",
        "nltk.download(\"stopwords\", quiet=True, download_dir=NLTK_DATA_DIR)\n",
        "nltk.download(\"wordnet\", quiet=True, download_dir=NLTK_DATA_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WJ0Of8zHEig"
      },
      "source": [
        "## Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkKL2Ly0E18u"
      },
      "outputs": [],
      "source": [
        "def make_query(claim: str) -> str:\n",
        "    \"\"\"\n",
        "    Apply basic preprocessing to convert a claim into a keyword-based search query.\n",
        "    \"\"\"\n",
        "    print(f\"Generating query from text: '{claim}'\")\n",
        "\n",
        "    # normalization\n",
        "    text = claim.lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
        "\n",
        "    # tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
        "\n",
        "    # lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    lemmas = list(dict.fromkeys(lemmas))\n",
        "\n",
        "    query = \" \".join(lemmas)\n",
        "    return query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1itV65OHMyP"
      },
      "source": [
        "## Image Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfcj4NUviVC7"
      },
      "outputs": [],
      "source": [
        "def caption(image_path: str, text_claim: Optional[str], model: str):\n",
        "    \"\"\"\n",
        "    Generate descriptive text claims from an image claim. If text claim already exists, add more context from image.\n",
        "    \"\"\"\n",
        "    print(f\"Generating query from image: '{image_path}'\")\n",
        "\n",
        "    with open(image_path, \"rb\") as f:\n",
        "        image_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Transform this image/text claim into a concise fact-checking search query:\n",
        "\n",
        "STEPS:\n",
        "- Identify main entities (people, organizations, events)\n",
        "- Remove question words (has, did, is, etc.)\n",
        "- Remove opinions and emotional language\n",
        "- Keep only factual core elements\n",
        "- Join with spaces as a search phrase\n",
        "\n",
        "TEXT CLAIM: \"{text_claim}\"\n",
        "\n",
        "SEARCH QUERY:\n",
        "\"\"\"\n",
        "\n",
        "    response = requests.post(\n",
        "        \"http://localhost:11434/api/generate\",\n",
        "        json={\"model\": model, \"prompt\": prompt, \"images\": [image_b64], \"stream\": False},\n",
        "    )\n",
        "\n",
        "    return response.json().get(\"response\", \"\").strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKQ2K_VLRtq2"
      },
      "source": [
        "# RETRIEVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSGyVJniRJcu"
      },
      "outputs": [],
      "source": [
        "def search(query: str, num_results: int) -> list[str]:\n",
        "    \"\"\"\n",
        "    Retrieve URLs using Google Custom Search API.\n",
        "    Return a list of string URLs.\n",
        "    \"\"\"\n",
        "    print(f\"Searching with query: '{query}'\")\n",
        "\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=API_KEY)\n",
        "    res = service.cse().list(q=query, cx=CSE_ID, num=num_results).execute()\n",
        "    urls = []\n",
        "    for item in res.get(\"items\", []):\n",
        "        urls.append(item[\"link\"])\n",
        "\n",
        "    print(f\"Found {len(urls)} URLs\")\n",
        "    return urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnHS3Ig_ClGz"
      },
      "outputs": [],
      "source": [
        "def fetch_text(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Fetch article text from a given URL.\n",
        "    Return the string body text from the HTML content.\n",
        "    \"\"\"\n",
        "    print(f\"Fetching article with URL: {url}\")\n",
        "    try:\n",
        "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)\n",
        "        if r.status_code != 200:\n",
        "            print(f\"    Failed to fetch: ({r.status_code})\")\n",
        "            return \"\"\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        ps = soup.select(\"article p, .entry-content p, p\")\n",
        "        text = \" \".join(p.get_text(strip=True) for p in ps)\n",
        "        return text if text.strip() else \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"    Error while fetching: {e}\")\n",
        "        return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_jgtXiNRHDM"
      },
      "outputs": [],
      "source": [
        "def retrieve(urls: list[str]) -> list[tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Retrieve documents from search results based on a query.\n",
        "    Return list of (url, text) tuples.\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    for url in urls:\n",
        "        document_text = fetch_text(url)\n",
        "        if document_text.strip():\n",
        "            documents.append((url, document_text))\n",
        "\n",
        "    # print(f\"Successfully retrieved {len(documents)} documents.\\n\")\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohUIp22PTyzM"
      },
      "source": [
        "# EMBEDDING AND RERANKING\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTpFxhjkHUux"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiYnGkNxxiIq"
      },
      "outputs": [],
      "source": [
        "def embed(\n",
        "    query: str, documents: list[tuple[str, str]], model_name: str\n",
        ") -> tuple[torch.Tensor, torch.Tensor, list[str], list[str]]:\n",
        "    \"\"\"\n",
        "    Generate embeddings for a query and a list of documents using Ollama.\n",
        "    Return query_embeddings, document_embeddings, urls, document_texts\n",
        "    \"\"\"\n",
        "    if not documents:\n",
        "        raise RuntimeError(\"No documents provided for embedding.\")\n",
        "\n",
        "    print(f\"Embedding {len(documents)} documents...\")\n",
        "\n",
        "    def ollama_embed(text: str):\n",
        "        try:\n",
        "            res = requests.post(\n",
        "                \"http://localhost:11434/api/embeddings\",\n",
        "                json={\"model\": model_name, \"prompt\": text},\n",
        "            )\n",
        "            data = res.json()\n",
        "            return data.get(\"embedding\", [])\n",
        "        except Exception as e:\n",
        "            print(f\"    Error getting embedding: {e}\")\n",
        "            return []\n",
        "\n",
        "    # query embedding\n",
        "    print(\"Generating query embedding...\")\n",
        "    query_vec = ollama_embed(query)\n",
        "    if not query_vec:\n",
        "        raise RuntimeError(\"Query embedding failed.\")\n",
        "\n",
        "    expected_dim = len(query_vec)\n",
        "    print(f\"Query embedding dimension: {expected_dim}\")\n",
        "\n",
        "    # document embeddings\n",
        "    doc_vecs = []\n",
        "    valid_urls = []\n",
        "    valid_texts = []\n",
        "\n",
        "    for url, text in documents:\n",
        "        vec = ollama_embed(text)\n",
        "        if not vec:\n",
        "            print(f\"    WARNING: Empty embedding for document. Skipping\")\n",
        "            continue\n",
        "        if len(vec) != expected_dim:\n",
        "            print(f\"    WARNING: Embedding dimension mismatch ({len(vec)} vs {expected_dim}). Skipping.\")\n",
        "            continue\n",
        "        doc_vecs.append(vec)\n",
        "        valid_urls.append(url)\n",
        "        valid_texts.append(text)\n",
        "\n",
        "    if not doc_vecs:\n",
        "        raise RuntimeError(\"No valid document embeddings generated.\")\n",
        "    print(f\"Successfully embedded {len(doc_vecs)} documents.\\n\")\n",
        "\n",
        "    query_embeddings = torch.tensor(np.array([query_vec]), dtype=torch.float32)\n",
        "    document_embeddings = torch.tensor(np.array(doc_vecs), dtype=torch.float32)\n",
        "\n",
        "    return query_embeddings, document_embeddings, valid_urls, valid_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liasyPJbHXvk"
      },
      "source": [
        "## Reranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGiFoI9uTvoU"
      },
      "outputs": [],
      "source": [
        "def rerank(\n",
        "    query_embeddings: Optional[torch.Tensor],\n",
        "    document_embeddings: Optional[torch.Tensor],\n",
        "    urls: list[str],\n",
        "    document_texts: list[str],\n",
        "    top_k: int = 3,\n",
        ") -> list[tuple[str, str, float]]:\n",
        "    \"\"\"\n",
        "    Rerank precomputed embeddings using cosine similarity.\n",
        "    Return a list of (url, text, score) tuples sorted by relevance score.\n",
        "    \"\"\"\n",
        "    if query_embeddings is None or document_embeddings is None:\n",
        "        raise RuntimeError(\"Query or document embeddings not found.\")\n",
        "    print(f\"Reranking {len(document_texts)} documents...\")\n",
        "\n",
        "    query_norm = query_embeddings / query_embeddings.norm(dim=1, keepdim=True)\n",
        "    doc_norms = document_embeddings / document_embeddings.norm(dim=1, keepdim=True)\n",
        "\n",
        "    scores = torch.mm(query_norm, doc_norms.T)[0].cpu().numpy()\n",
        "\n",
        "    ranked = list(zip(urls, document_texts, scores))\n",
        "    ranked.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    return ranked[:top_k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FgRCDHzbNYi"
      },
      "source": [
        "# PREMISE GENERATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOvQJObrbQTT"
      },
      "outputs": [],
      "source": [
        "def generate_premise(claim: str, documents: list[str], model: str):\n",
        "    \"\"\"\n",
        "    Summarize the evidence retrieved for a claim into a short premise.\n",
        "    \"\"\"\n",
        "    joined_documents = \"\\n\".join([f\"- {document}\" for document in documents])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a factual summarization assistant. Your task is to extract and summarize ONLY the factual content from the provided documents to create a premise for fact-checking.\n",
        "\n",
        "**INSTRUCTIONS:**\n",
        "1. Read the claim and the supporting documents carefully\n",
        "2. Extract ONLY factual information from the documents that are relevant to verifying the claim\n",
        "3. Summarize these facts concisely into a single premise\n",
        "4. DO NOT include any analysis, conclusions, or opinions\n",
        "5. DO NOT reference the documents themselves or use phrases like \"according to the articles\"\n",
        "6. Present ONLY the factual premise\n",
        "\n",
        "**CLAIM TO FACT-CHECK:**\n",
        "\"{claim}\"\n",
        "\n",
        "**SUPPORTING DOCUMENTS:**\n",
        "{joined_documents}\n",
        "\n",
        "**OUTPUT FORMAT:**\n",
        "Provide only the factual premise without any introductory text, bullet points, or numbering.\n",
        "\"\"\"\n",
        "\n",
        "    response = requests.post(\n",
        "        \"http://localhost:11434/api/generate\",\n",
        "        json={\"model\": model, \"prompt\": prompt, \"stream\": False},\n",
        "    )\n",
        "\n",
        "    return response.json().get(\"response\", \"\").strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NJYZPteT1ZC"
      },
      "source": [
        "# PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h66pwXz-S8uw"
      },
      "outputs": [],
      "source": [
        "def pipeline(text_claim: str, image_path: Optional[str]) -> list[tuple[str, str, float]]:\n",
        "    \"\"\"\n",
        "    Complete IR pipeline: retrieve, rerank, and return top documents.\n",
        "    Return a list of (url, text, score) tuples for top_k most relevant documents.\n",
        "    \"\"\"\n",
        "    # parameters\n",
        "    ollama_emb_name = \"qwen3-embedding:0.6b\"\n",
        "    ollama_llm_name = \"qwen3:0.6b\"\n",
        "    ollama_vlm_name = \"qwen2.5vl:3b\"\n",
        "    num_results = 5\n",
        "    top_k = 2\n",
        "\n",
        "    # preprocessing\n",
        "    if image_path:\n",
        "        image_claim = caption(image_path, text_claim, ollama_vlm_name)\n",
        "        query = make_query(image_claim)\n",
        "        if not text_claim.strip():\n",
        "            text_claim = image_claim # if image only, use response as text claim\n",
        "    else:\n",
        "        query = make_query(text_claim)\n",
        "\n",
        "    # retrieval\n",
        "    urls = search(query, num_results)\n",
        "\n",
        "    documents = retrieve(urls)\n",
        "    if not documents:\n",
        "        premise = \"No documents related to the claim were found.\"\n",
        "        return premise\n",
        "\n",
        "    # embedding\n",
        "    claim_embeddings, document_embeddings, urls, document_texts = embed(text_claim, documents, ollama_emb_name)\n",
        "\n",
        "    # reranking\n",
        "    best_docs = rerank(claim_embeddings, document_embeddings, urls, document_texts, top_k)\n",
        "    for url, text, score in best_docs:\n",
        "        print(f\"{url}\\nScore: {score:.3f}\\nText: {text[101:301]}\\n\")\n",
        "\n",
        "    # premise generation\n",
        "    premise = generate_premise(text_claim, best_docs, ollama_llm_name)\n",
        "\n",
        "    # get best document URLs\n",
        "    best_doc_urls = [url for url, _, _ in best_docs]\n",
        "\n",
        "    # return premise and best document URLs\n",
        "    return premise, best_doc_urls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOKzLsGI_oaw"
      },
      "source": [
        "## Test Claim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXHlqJ2pSl3Y"
      },
      "source": [
        "**Notes**\n",
        "\n",
        "*   Claims in English are processed better than claims in Filipino. Seek more robust (maybe multilingual-LLM-based) solutions as a possible optimization step.\n",
        "\n",
        "*   With limited testing, decomposing claims into multiple subclaims have yet to prove useful. It multiplies the processing time (2-5x), but Google SEO seems to be powerful enough with just one query.\n",
        "   \n",
        "*   Better reranking scores (~20%) when using multilingual embedding models for claims/documents in Filipino. Multilingual models allow for shared embedding spaces across languages, e.g. mixed English/Filipino documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "claim = \"\"\n",
        "image_path = \"./datasets/documents/1.png\"\n",
        "link = \"https://www.factrakers.org/post/vico-sotto-atasha-muhlach-pregnancy-hoax-resurfaces\"\n",
        "\n",
        "print(\"\\nProcessing claim...\\n\")\n",
        "generated_premise, retrieved_urls = pipeline(claim, image_path)\n",
        "print(\"\\nFinished processing!\\n\")\n",
        "\n",
        "print(f\"Generated Premise:\\n{generated_premise}\\n\")\n",
        "print(f\"Retrieved URLs:\\n{\"\".join(url + '\\n' for url in retrieved_urls)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_df(path, output_dir=\"datasets/extracted_images\"):\n",
        "    wb = load_workbook(path)\n",
        "    ws = wb.active\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # load text\n",
        "    data = []\n",
        "    headers = [cell.value for cell in ws[1]]\n",
        "    for row in ws.iter_rows(min_row=2, values_only=True):\n",
        "        data.append(list(row))\n",
        "    df = pd.DataFrame(data, columns=headers)\n",
        "    df[\"Image Path\"] = None\n",
        "\n",
        "    # extract images and map to row numbers\n",
        "    for idx, img in enumerate(ws._images):\n",
        "        anchor = img.anchor\n",
        "        if isinstance(anchor, str):\n",
        "            ref = anchor\n",
        "        elif hasattr(anchor, \"_from\"):\n",
        "            ref = f\"{chr(anchor._from.col + 65)}{anchor._from.row + 1}\"\n",
        "        else:\n",
        "            ref = None\n",
        "\n",
        "        img_bytes = io.BytesIO(img._data())\n",
        "        pil_img = Image.open(img_bytes)\n",
        "\n",
        "        filename = f\"img_{idx+1}_{ref or 'unknown'}.png\"\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "        pil_img.save(filepath)\n",
        "\n",
        "        # match image to row\n",
        "        if ref:\n",
        "            try:\n",
        "                row_num = int(''.join(filter(str.isdigit, ref)))\n",
        "                df.loc[row_num - 2, \"Image Path\"] = filepath\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_dataset(pipeline, df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \n",
        "    def process_row(row, pipeline) -> pd.Series:\n",
        "        claim = row.get(\"Hypothesis/Claims\")\n",
        "        image_path = row.get(\"Image Path\")\n",
        "        link = row.get(\"Link\")\n",
        "\n",
        "        # validate inputs\n",
        "        if not isinstance(claim, str) or not claim.strip():\n",
        "            return pd.Series({\n",
        "                'Generated Premise': None,\n",
        "                'Match': False,\n",
        "                'Retrieved_URLs': []\n",
        "            })\n",
        "        if not (isinstance(image_path, str) and os.path.exists(image_path)):\n",
        "            image_path = None\n",
        "\n",
        "        # pipeline execution\n",
        "        try:\n",
        "            premise, urls = pipeline(claim, image_path)\n",
        "            urls = urls or []\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing claim '{claim[:50]}...': {e}\")\n",
        "            premise, urls = None, []\n",
        "\n",
        "        # check for matching urls\n",
        "        match = False\n",
        "        if link and urls:\n",
        "            normalized_link = link.lower().strip()\n",
        "            match = any(normalized_link == url.lower().strip() for url in urls)\n",
        "        \n",
        "        # print premise\n",
        "        print(f\"Generated Premise: '{premise}'\")\n",
        "        print(f\"---- ---- ---- ---- ---- ----\")\n",
        "\n",
        "        return pd.Series({\n",
        "            'Generated Premise': premise,\n",
        "            'Match': match,\n",
        "            'Retrieved_URLs': urls\n",
        "        })\n",
        "    \n",
        "    # process and merge original df\n",
        "    result_df = df.apply(lambda row: process_row(row, pipeline), axis=1)\n",
        "    df[[\"Generated Premise\", \"Match\", \"Retrieved_URLs\"]] = result_df\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_df = get_df(\"datasets/sheets/dataset.xlsx\")\n",
        "input_df = input_df.head(10)\n",
        "\n",
        "output_df = process_dataset(pipeline, input_df)\n",
        "print(\"\\nDataset processing complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_name = os.path.basename(__file__)\n",
        "output_path = f\"/output/{pipeline_name}.xlsx\"\n",
        "\n",
        "output_df.to_excel(output_path, index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "patunai-ir",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
