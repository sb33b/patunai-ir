{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSjtDYkkXDXw"
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvqO31oL_T7c"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_5hjYjdCc-r"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import concurrent\n",
    "import io\n",
    "import shutil\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "import ipynbname\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.spreadsheet_drawing import OneCellAnchor, TwoCellAnchor\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyjvDjvIOPtl"
   },
   "outputs": [],
   "source": [
    "# get .env keys\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "CSE_ID = os.getenv(\"CSE_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"OLLAMA_EMBED_MODEL\": \"qwen3-embedding:0.6b\",\n",
    "    \"OLLAMA_LLM_MODEL\": \"qwen3:1.7b\",\n",
    "    \"OLLAMA_VLM_MODEL\": \"qwen3-vl:2b\",\n",
    "    \"SEARCH_NUM_RESULTS\": 5,\n",
    "    \"RERANK_TOP_K\": 2,\n",
    "    \"IMAGE_SIZE\": (672, 672)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = os.path.join(os.getcwd(), \"cache\")\n",
    "WEB_CACHE_FILE = os.path.join(CACHE_DIR, \"web_content_cache.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUGBqya9_ZsS"
   },
   "source": [
    "## Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQ-ckmscXtN0"
   },
   "outputs": [],
   "source": [
    "# check for Ollama instance\n",
    "for i in range(30):\n",
    "    try:\n",
    "        r = requests.get(\"http://localhost:11434/api/tags\", timeout=1)\n",
    "        if r.status_code == 200:\n",
    "            print(\"Ollama served at http://localhost:11434/\")\n",
    "            break\n",
    "    except Exception:\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    raise RuntimeError(\"Ollama failed to start.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwDXmXTgF2op"
   },
   "source": [
    "# CLAIM PROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "W_rEm6En59Xq"
   },
   "outputs": [],
   "source": [
    "# download nltk resources\n",
    "NLTK_DATA_DIR = os.path.join(os.getcwd(), \"cache/nltk_data\")\n",
    "os.makedirs(NLTK_DATA_DIR, exist_ok=True)\n",
    "nltk.data.path.append(NLTK_DATA_DIR)\n",
    "nltk.download(\"punkt\", quiet=True, download_dir=NLTK_DATA_DIR)\n",
    "nltk.download(\"punkt_tab\", quiet=True, download_dir=NLTK_DATA_DIR)\n",
    "nltk.download(\"stopwords\", quiet=True, download_dir=NLTK_DATA_DIR)\n",
    "nltk.download(\"wordnet\", quiet=True, download_dir=NLTK_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ollama(payload, url=\"http://localhost:11434/api/generate\", retries=3):\n",
    "    \"\"\"\n",
    "    Get response from the Ollama server for LLM/VLM inference.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.post(url, json=payload, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                print(f\"Ollama failed after {retries} attempts: {e}\")\n",
    "                return {}\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WJ0Of8zHEig"
   },
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkKL2Ly0E18u"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(claim: str) -> str:\n",
    "    \"\"\"\n",
    "    Apply basic preprocessing to convert a claim into a keyword-based search query.\n",
    "    \"\"\"\n",
    "    print(f\"Generating query from text: '{claim}'\")\n",
    "\n",
    "    # normalization\n",
    "    text = claim.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "\n",
    "    # tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "\n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    lemmas = list(dict.fromkeys(lemmas))\n",
    "\n",
    "    query = \" \".join(lemmas)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1itV65OHMyP"
   },
   "source": [
    "## Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path: str, target_size: tuple = (672, 672)) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses an image for VLM analysis with resizing and letterboxing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # image resampling\n",
    "        processed_img = ImageOps.pad(img, target_size, method=Image.Resampling.LANCZOS, color=(0, 0, 0), centering=(0.5, 0.5))\n",
    "\n",
    "        base, ext = os.path.splitext(image_path)\n",
    "        output_path = f\"{base}_processed{ext}\"\n",
    "        processed_img.save(output_path, quality=95)\n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing image '{image_path}': {e}\")\n",
    "        return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfcj4NUviVC7"
   },
   "outputs": [],
   "source": [
    "def caption(image_path: str, text_claim: Optional[str], model: str):\n",
    "    \"\"\"\n",
    "    Generate descriptive text claims from an image claim. If text claim already exists, add more context from image.\n",
    "    \"\"\"\n",
    "    print(f\"Generating query from image: '{image_path}'\")\n",
    "\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Transform this image/text claim into a concise fact-checking search query:\n",
    "\n",
    "STEPS:\n",
    "- Identify main entities (people, organizations, events)\n",
    "- Remove question words (has, did, is, etc.)\n",
    "- Remove opinions and emotional language\n",
    "- Keep only factual core elements\n",
    "- Join with spaces as a search phrase\n",
    "- Only respond with the plain search query\n",
    "\n",
    "TEXT CLAIM: \"{text_claim}\"\n",
    "\n",
    "SEARCH QUERY:\n",
    "\"\"\"\n",
    "\n",
    "    response_json = query_ollama({\"model\": model, \"prompt\": prompt, \"images\": [image_b64], \"stream\": False})\n",
    "    return response_json.get(\"response\", \"\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKQ2K_VLRtq2"
   },
   "source": [
    "# RETRIEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_web_cache() -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load cached articles from directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "    if os.path.exists(WEB_CACHE_FILE):\n",
    "        with open(WEB_CACHE_FILE, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                return json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Warning: Web cache file is corrupted. Starting new cache.\")\n",
    "                return {}\n",
    "    return {}\n",
    "\n",
    "def save_web_cache(cache: dict[str, str]):\n",
    "    \"\"\"\n",
    "    Save cached articles to directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(WEB_CACHE_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cache, f, indent=4)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving web cache: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSGyVJniRJcu"
   },
   "outputs": [],
   "source": [
    "def search(query: str, num_results: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve URLs using Google Custom Search API.\n",
    "    Return a list of string URLs.\n",
    "    \"\"\"\n",
    "    print(f\"Searching with query: '{query}'\")\n",
    "\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=API_KEY)\n",
    "    res = service.cse().list(q=query, cx=CSE_ID, num=num_results).execute()\n",
    "    urls = []\n",
    "    for item in res.get(\"items\", []):\n",
    "        urls.append(item[\"link\"])\n",
    "\n",
    "    print(f\"Found {len(urls)} URLs\")\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnHS3Ig_ClGz"
   },
   "outputs": [],
   "source": [
    "def fetch_text(url: str, cache: dict[str, str], cache_update_flag: list[bool]) -> str:\n",
    "    \"\"\"\n",
    "    Fetch article text from a given URL.\n",
    "    Return the string body text from the HTML content.\n",
    "    \"\"\"    \n",
    "    if url in cache:\n",
    "        print(f\"Fetching article with URL: {url} (CACHED)\")\n",
    "        return cache[url]\n",
    "    \n",
    "    print(f\"Fetching article with URL: {url} (SCRAPING)\")\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "        r = requests.get(url, headers=headers, timeout=5)\n",
    "        if r.status_code != 200:\n",
    "            return \"\"\n",
    "            \n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        for script in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "            script.decompose()\n",
    "\n",
    "        content = soup.find('article') or soup.find('div', class_='entry-content') or soup.find('main') or soup\n",
    "        text = content.get_text(separator=' ', strip=True)\n",
    "        text = re.sub(r'\\\\s+', ' ', text)\n",
    "        \n",
    "        if text:\n",
    "            cache[url] = text\n",
    "            cache_update_flag[0] = True\n",
    "            \n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"    Error while fetching: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_jgtXiNRHDM"
   },
   "outputs": [],
   "source": [
    "def retrieve(urls: list[str]) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Retrieve documents from search results based on a query.\n",
    "    \"\"\"\n",
    "    cache = load_web_cache()\n",
    "    cache_update_flag = [False]\n",
    "    def fetch_with_cache(url):\n",
    "        return fetch_text(url, cache, cache_update_flag)\n",
    "    \n",
    "    documents = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_to_url = {executor.submit(fetch_with_cache, url): url for url in urls}\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                text = future.result()\n",
    "                if text:\n",
    "                    documents.append((url, text))\n",
    "            except Exception as e:\n",
    "                print(f\"    Error processing {url}: {e}\")\n",
    "    \n",
    "    if cache_update_flag[0]:\n",
    "        print(\"Saving updated web content cache...\")\n",
    "        save_web_cache(cache)\n",
    "        \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohUIp22PTyzM"
   },
   "source": [
    "# EMBEDDING AND RERANKING\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTpFxhjkHUux"
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UiYnGkNxxiIq"
   },
   "outputs": [],
   "source": [
    "def embed(\n",
    "    query: str, documents: list[tuple[str, str]], model_name: str\n",
    ") -> tuple[torch.Tensor, torch.Tensor, list[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a query and a list of documents using Ollama.\n",
    "    Return query_embeddings, document_embeddings, urls, document_texts\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        raise RuntimeError(\"No documents provided for embedding.\")\n",
    "\n",
    "    print(f\"Embedding {len(documents)} documents...\")\n",
    "\n",
    "    def ollama_embed(text: str):\n",
    "        try:\n",
    "            res = requests.post(\n",
    "                \"http://localhost:11434/api/embeddings\",\n",
    "                json={\"model\": model_name, \"prompt\": text},\n",
    "            )\n",
    "            data = res.json()\n",
    "            return data.get(\"embedding\", [])\n",
    "        except Exception as e:\n",
    "            print(f\"    Error getting embedding: {e}\")\n",
    "            return []\n",
    "\n",
    "    # query embedding\n",
    "    print(\"Generating query embedding...\")\n",
    "    query_vec = ollama_embed(query)\n",
    "    if not query_vec:\n",
    "        raise RuntimeError(\"Query embedding failed.\")\n",
    "\n",
    "    expected_dim = len(query_vec)\n",
    "    print(f\"Query embedding dimension: {expected_dim}\")\n",
    "\n",
    "    # document embeddings\n",
    "    doc_vecs = []\n",
    "    valid_urls = []\n",
    "    valid_texts = []\n",
    "\n",
    "    for url, text in documents:\n",
    "        vec = ollama_embed(text)\n",
    "        if not vec:\n",
    "            print(f\"    WARNING: Empty embedding for document. Skipping\")\n",
    "            continue\n",
    "        if len(vec) != expected_dim:\n",
    "            print(f\"    WARNING: Embedding dimension mismatch ({len(vec)} vs {expected_dim}). Skipping.\")\n",
    "            continue\n",
    "        doc_vecs.append(vec)\n",
    "        valid_urls.append(url)\n",
    "        valid_texts.append(text)\n",
    "\n",
    "    if not doc_vecs:\n",
    "        raise RuntimeError(\"No valid document embeddings generated.\")\n",
    "    print(f\"Successfully embedded {len(doc_vecs)} documents.\\n\")\n",
    "\n",
    "    query_embeddings = torch.tensor(np.array([query_vec]), dtype=torch.float32)\n",
    "    document_embeddings = torch.tensor(np.array(doc_vecs), dtype=torch.float32)\n",
    "\n",
    "    return query_embeddings, document_embeddings, valid_urls, valid_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liasyPJbHXvk"
   },
   "source": [
    "## Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGiFoI9uTvoU"
   },
   "outputs": [],
   "source": [
    "def rerank(\n",
    "    query_embeddings: Optional[torch.Tensor],\n",
    "    document_embeddings: Optional[torch.Tensor],\n",
    "    urls: list[str],\n",
    "    document_texts: list[str],\n",
    "    top_k: int = 3,\n",
    ") -> list[tuple[str, str, float]]:\n",
    "    \"\"\"\n",
    "    Rerank precomputed embeddings using cosine similarity.\n",
    "    Return a list of (url, text, score) tuples sorted by relevance score.\n",
    "    \"\"\"\n",
    "    if query_embeddings is None or document_embeddings is None:\n",
    "        raise RuntimeError(\"Query or document embeddings not found.\")\n",
    "    print(f\"Reranking {len(document_texts)} documents...\")\n",
    "\n",
    "    query_norm = query_embeddings / query_embeddings.norm(dim=1, keepdim=True)\n",
    "    doc_norms = document_embeddings / document_embeddings.norm(dim=1, keepdim=True)\n",
    "\n",
    "    scores = torch.mm(query_norm, doc_norms.T)[0].cpu().numpy()\n",
    "\n",
    "    ranked = list(zip(urls, document_texts, scores))\n",
    "    ranked.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    return ranked[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FgRCDHzbNYi"
   },
   "source": [
    "# PREMISE GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOvQJObrbQTT"
   },
   "outputs": [],
   "source": [
    "def generate_premise(claim: str, documents: list[str], model: str):\n",
    "    \"\"\"\n",
    "    Summarize the evidence retrieved for a claim into a short premise.\n",
    "    \"\"\"\n",
    "    joined_documents = \"\\n\".join([f\"- {document}\" for document in documents])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a factual summarization assistant. Your task is to extract and summarize ONLY the factual content from the provided documents to create a premise for fact-checking.\n",
    "\n",
    "STEPS:\n",
    "- Read the claim and the supporting documents carefully\n",
    "- Extract ONLY factual information from the documents that are relevant to verifying the claim\n",
    "- Summarize these facts concisely into a single premise\n",
    "- DO NOT include any analysis, conclusions, or opinions\n",
    "- DO NOT reference the documents themselves or use phrases like \"according to the articles\"\n",
    "- Present only the factual premise\n",
    "\n",
    "CLAIM: \"{claim}\"\n",
    "\n",
    "DOCUMENTS:\n",
    "{joined_documents}\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Provide only the factual premise without any introductory text, bullet points, or numbering.\n",
    "\"\"\"\n",
    "\n",
    "    response_json = query_ollama({\"model\": model, \"prompt\": prompt, \"stream\": False})\n",
    "    return response_json.get(\"response\", \"\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NJYZPteT1ZC"
   },
   "source": [
    "# PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h66pwXz-S8uw"
   },
   "outputs": [],
   "source": [
    "def pipeline(text_claim: str, image_path: Optional[str]) -> list[tuple[str, str, float]]:\n",
    "    \"\"\"\n",
    "    Complete IR pipeline: retrieve, rerank, and return top documents.\n",
    "    Return a list of (url, text, score) tuples for top_k most relevant documents.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # parameters\n",
    "    ollama_emb_name = CONFIG[\"OLLAMA_EMBED_MODEL\"]\n",
    "    ollama_llm_name = CONFIG[\"OLLAMA_LLM_MODEL\"]\n",
    "    ollama_vlm_name = CONFIG[\"OLLAMA_VLM_MODEL\"]\n",
    "    num_results = CONFIG[\"SEARCH_NUM_RESULTS\"]\n",
    "    top_k = CONFIG[\"RERANK_TOP_K\"]\n",
    "    image_size = CONFIG[\"IMAGE_SIZE\"]\n",
    "\n",
    "    # preprocessing\n",
    "    if image_path:\n",
    "        # image preprocessing\n",
    "        processed_image_path = preprocess_image(image_path, image_size)\n",
    "        final_image_path = processed_image_path if processed_image_path else image_path\n",
    "\n",
    "        # image captioning\n",
    "        image_claim = caption(final_image_path, text_claim, ollama_vlm_name)\n",
    "\n",
    "        # vlm postprocessing\n",
    "        query = preprocess_text(image_claim)\n",
    "        \n",
    "        if not text_claim.strip():\n",
    "            text_claim = image_claim # if image only, use query as text claim\n",
    "    else:\n",
    "        # text preprocessing\n",
    "        query = preprocess_text(text_claim)\n",
    "\n",
    "    # retrieval\n",
    "    urls = search(query, num_results)\n",
    "\n",
    "    documents = retrieve(urls)\n",
    "    if not documents:\n",
    "        premise = \"No documents related to the claim were found.\"\n",
    "        return premise, []\n",
    "\n",
    "    # embedding\n",
    "    claim_embeddings, document_embeddings, urls, document_texts = embed(text_claim, documents, ollama_emb_name)\n",
    "\n",
    "    # reranking\n",
    "    best_docs = rerank(claim_embeddings, document_embeddings, urls, document_texts, top_k)\n",
    "\n",
    "    # optional: show reranked document contents\n",
    "    for url, text, score in best_docs:\n",
    "        print(f\"{url}\\nScore: {score:.3f}\\nText: {text[:301]}\\n\")\n",
    "\n",
    "    # premise generation\n",
    "    premise = generate_premise(text_claim, best_docs, ollama_llm_name)\n",
    "\n",
    "    # get best document URLs\n",
    "    best_doc_urls = [url for url, _, _ in best_docs]\n",
    "\n",
    "    # return premise and best document URLs\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Completed in {duration} s\")\n",
    "\n",
    "    return premise, best_doc_urls, duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOKzLsGI_oaw"
   },
   "source": [
    "## Test Claim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXHlqJ2pSl3Y"
   },
   "source": [
    "**Notes**\n",
    "\n",
    "*   Claims in English are processed better than claims in Filipino. Seek more robust (maybe multilingual-LLM-based) solutions as a possible optimization step.\n",
    "\n",
    "*   With limited testing, decomposing claims into multiple subclaims have yet to prove useful. It multiplies the processing time (2-5x), but Google SEO seems to be powerful enough with just one query.\n",
    "   \n",
    "*   Better reranking scores (~20%) when using multilingual embedding models for claims/documents in Filipino. Multilingual models allow for shared embedding spaces across languages, e.g. mixed English/Filipino documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claim = \"\"\n",
    "# image_path = \"../datasets/documents/1.png\"\n",
    "# link = \"https://www.factrakers.org/post/vico-sotto-atasha-muhlach-pregnancy-hoax-resurfaces\"\n",
    "\n",
    "# print(\"\\nProcessing claim...\\n\")\n",
    "# generated_premise, retrieved_urls, duration = pipeline(claim, image_path)\n",
    "# print(\"\\nFinished processing!\\n\")\n",
    "\n",
    "# print(f\"Generated Premise:\\n{generated_premise}\\n\")\n",
    "# print(f\"Retrieved URLs:\\n{\"\".join(url + '\\n' for url in retrieved_urls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(path, output_dir=\"data/extracted_images\"):\n",
    "    wb = load_workbook(path)\n",
    "    ws = wb.active\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    data = []\n",
    "    headers = [cell.value for cell in ws[1]]\n",
    "    for row in ws.iter_rows(min_row=2, values_only=True):\n",
    "        data.append(list(row))\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    df[\"Image Path\"] = None\n",
    "\n",
    "    for idx, img in enumerate(ws._images):\n",
    "        anchor = img.anchor\n",
    "        if isinstance(anchor, str):\n",
    "            ref = anchor\n",
    "        elif hasattr(anchor, \"_from\"):\n",
    "            ref = f\"{chr(anchor._from.col + 65)}{anchor._from.row + 1}\"\n",
    "        else:\n",
    "            ref = None\n",
    "\n",
    "        img_bytes = io.BytesIO(img._data())\n",
    "        pil_img = Image.open(img_bytes)\n",
    "\n",
    "        filename = f\"img_{idx+1}_{ref or 'unknown'}.png\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        pil_img.save(filepath)\n",
    "\n",
    "        if ref:\n",
    "            try:\n",
    "                row_num = int(''.join(filter(str.isdigit, ref)))\n",
    "                target_row_num = row_num\n",
    "                if row_num >= 3 and row_num % 2 != 0:\n",
    "                    target_row_num = row_num - 1\n",
    "                df_index = target_row_num - 2\n",
    "                if df_index >= 0 and df_index < len(df):\n",
    "                    df.loc[df_index, \"Image Path\"] = filepath\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    for i in range(1, len(df), 2):\n",
    "        current_image_path = df.loc[i, \"Image Path\"]\n",
    "        if pd.isna(current_image_path):\n",
    "            prev_claim_path = df.loc[i - 1, \"Image Path\"]\n",
    "            \n",
    "            # preceding claim row has an image\n",
    "            if pd.notna(prev_claim_path):\n",
    "                base_name = os.path.basename(prev_claim_path)\n",
    "                name, ext = os.path.splitext(base_name)\n",
    "                fact_row_num = i + 2\n",
    "                new_filename = f\"{name}_copied_R{fact_row_num}{ext}\"\n",
    "                new_filepath = os.path.join(output_dir, new_filename)\n",
    "                shutil.copy2(prev_claim_path, new_filepath)\n",
    "                df.loc[i, \"Image Path\"] = new_filepath\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_excel_row(\n",
    "    path: str,\n",
    "    row_index: int,\n",
    "    premise: Optional[str],\n",
    "    match: bool,\n",
    "    urls: list[str],\n",
    "    duration: Optional[float]\n",
    "):\n",
    "    wb = load_workbook(path)\n",
    "    ws = wb.active\n",
    "    excel_row_num = row_index + 2 \n",
    "\n",
    "    headers = [cell.value for cell in ws[1]]\n",
    "    try:\n",
    "        COL_PREMISE = headers.index('Generated Premise') + 1\n",
    "    except ValueError:\n",
    "        start_col = len(headers) + 1\n",
    "        COL_PREMISE = start_col\n",
    "        COL_MATCH = start_col + 1\n",
    "        COL_URLS = start_col + 2\n",
    "        COL_DURATION = start_col + 3\n",
    "        ws.cell(row=1, column=COL_PREMISE, value='Generated Premise')\n",
    "        ws.cell(row=1, column=COL_MATCH, value='Match')\n",
    "        ws.cell(row=1, column=COL_URLS, value='Retrieved_URLs')\n",
    "        ws.cell(row=1, column=COL_DURATION, value='Duration')\n",
    "        \n",
    "    else:\n",
    "        COL_MATCH = COL_PREMISE + 1\n",
    "        COL_URLS = COL_PREMISE + 2\n",
    "        COL_DURATION = COL_PREMISE + 3\n",
    "\n",
    "    ws.cell(row=excel_row_num, column=COL_PREMISE, value=premise)\n",
    "    ws.cell(row=excel_row_num, column=COL_MATCH, value=match)\n",
    "    ws.cell(row=excel_row_num, column=COL_URLS, value=' | '.join(urls))\n",
    "    ws.cell(row=excel_row_num, column=COL_DURATION, value=duration)\n",
    "    wb.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(pipeline, input_df: pd.DataFrame, output_path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        if not os.path.exists(output_path):\n",
    "            input_df.to_excel(output_path, index=False)\n",
    "        output_df = pd.read_excel(output_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading/creating output Excel at {output_path}: {e}\")\n",
    "        output_df = input_df.copy()\n",
    "\n",
    "    for index, row in output_df.iterrows():\n",
    "        claim = row.get(\"Hypothesis/Claims\")\n",
    "        \n",
    "        if \"Generated Premise\" in output_df.columns and pd.notna(row.get(\"Generated Premise\")):\n",
    "            print(f\"Skipping row {index+2} (Claim: '{claim[:30]}...') - Already processed.\")\n",
    "            continue\n",
    "        print(f\"Processing row {index+2} (Claim: '{claim[:30]}...')\")\n",
    "\n",
    "        image_path = row.get(\"Image Path\")\n",
    "        link = row.get(\"Link\")\n",
    "        premise, urls, duration, match = None, [], None, False\n",
    "\n",
    "        if not isinstance(claim, str) or not claim.strip():\n",
    "            print(\"Skipping - Invalid or empty claim.\")\n",
    "            continue\n",
    "        if not (isinstance(image_path, str) and os.path.exists(image_path)):\n",
    "            image_path = None\n",
    "\n",
    "        try:\n",
    "            premise, urls, duration = pipeline(claim, image_path)\n",
    "            urls = urls or []\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing claim '{claim[:50]}...': {e}\")\n",
    "            \n",
    "        if link and urls:\n",
    "            normalized_link = link.lower().strip()\n",
    "            match = any(normalized_link == url.lower().strip() for url in urls)\n",
    "\n",
    "        print(f\"Generated Premise: '{premise}'\")\n",
    "        print(f\"---- ---- ---- ---- ---- ----\")\n",
    "        update_excel_row(output_path, index, premise, match, urls, duration)\n",
    "        \n",
    "    print(\"\\nDataset processing complete.\")\n",
    "    return pd.read_excel(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../datasets/sheets/dataset.xlsx\"\n",
    "input_df = get_df(input_path)\n",
    "\n",
    "nb_path = ipynbname.path()\n",
    "pipeline_name = os.path.splitext(os.path.basename(str(nb_path)))[0]\n",
    "OUTPUT_DIR = os.path.join(os.getcwd(), \"outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "output_path = os.path.join(OUTPUT_DIR, f\"{pipeline_name}.xlsx\")\n",
    "output_df = process_dataset(pipeline, input_df, output_path)\n",
    "print(f\"Final output saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "patunai-ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
