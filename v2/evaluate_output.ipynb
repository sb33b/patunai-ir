{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "227f7fa2",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dac11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a5fcc",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a02c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"outputs/Patunai-QwenModel.xlsx\"\n",
    "sheet_name = \"Text\"\n",
    "\n",
    "col_manual = \"Premise/Facts\"\n",
    "col_generated = \"Generated Premise\"\n",
    "\n",
    "model_list = [\n",
    "    \"all-distilroberta-v1\", # previous result models\n",
    "    \"all-MiniLM-L6-v2\",\n",
    "\n",
    "    \"text-embedding-3-large\", # OpenAI API models\n",
    "    \"text-embedding-3-small\",\n",
    "\n",
    "    \"BAAI/bge-base-en-v1.5\", # Explorative models\n",
    "    \"intfloat/e5-base-v2\",\n",
    "    \"paraphrase-mpnet-base-v2\",\n",
    "    \"all-mpnet-base-v2\",\n",
    "\n",
    "    \"Qwen/Qwen3-Embedding-0.6b\", # current model for reranking\n",
    "]\n",
    "\n",
    "df = pd.read_excel(input_path, sheet_name=sheet_name)\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d635a",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1669b62c",
   "metadata": {},
   "source": [
    "## Semantic Evaluator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d13401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, embedding_model: str):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        print(f\"Running on: {self.device}\")\n",
    "        print(f\"Embedding model: {self.embedding_model}\")\n",
    "\n",
    "        # Cache per (backend, model, text)\n",
    "        self._embedding_cache: dict[tuple[str, str, str], np.ndarray] = {}\n",
    "\n",
    "        # Lazy-loaded HF model\n",
    "        self._hf_model = None\n",
    "\n",
    "    # -------------------------\n",
    "    # Backend routing\n",
    "    # -------------------------\n",
    "    def _is_openai_model(self) -> bool:\n",
    "        return self.embedding_model.startswith(\"text-embedding-3-\")\n",
    "\n",
    "    def _get_hf_model(self) -> SentenceTransformer:\n",
    "        if self._hf_model is None:\n",
    "            print(f\"Loading SentenceTransformer ({self.embedding_model}) | \", end=\"\")\n",
    "            self._hf_model = SentenceTransformer(\n",
    "                self.embedding_model,\n",
    "                device=self.device\n",
    "            )\n",
    "        return self._hf_model\n",
    "\n",
    "    # -------------------------\n",
    "    # Embedding\n",
    "    # -------------------------\n",
    "    def _get_embedding(self, text: str) -> np.ndarray | None:\n",
    "        if not text:\n",
    "            return None\n",
    "\n",
    "        backend = \"openai\" if self._is_openai_model() else \"hf\"\n",
    "        cache_key = (backend, self.embedding_model, text)\n",
    "\n",
    "        if cache_key in self._embedding_cache:\n",
    "            return self._embedding_cache[cache_key]\n",
    "\n",
    "        if backend == \"openai\":\n",
    "            emb = self._get_openai_embedding(text)\n",
    "        else:\n",
    "            emb = self._get_hf_embedding(text)\n",
    "\n",
    "        self._embedding_cache[cache_key] = emb\n",
    "        return emb\n",
    "\n",
    "    def _get_openai_embedding(self, text: str) -> np.ndarray:\n",
    "        response = client.embeddings.create(\n",
    "            model=self.embedding_model,\n",
    "            input=text\n",
    "        )\n",
    "        return np.array(response.data[0].embedding, dtype=np.float32)\n",
    "\n",
    "    def _get_hf_embedding(self, text: str) -> np.ndarray:\n",
    "        model = self._get_hf_model()\n",
    "        emb = model.encode(\n",
    "            text,\n",
    "            normalize_embeddings=False,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        return emb.astype(np.float32)\n",
    "\n",
    "    # -------------------------\n",
    "    # Similarity\n",
    "    # -------------------------\n",
    "    def get_semantic_similarity(self, text1: str, text2: str) -> float:\n",
    "        if not text1 or not text2:\n",
    "            return 0.0\n",
    "\n",
    "        emb1 = self._get_embedding(text1)\n",
    "        emb2 = self._get_embedding(text2)\n",
    "\n",
    "        if emb1 is None or emb2 is None:\n",
    "            return 0.0\n",
    "\n",
    "        score = cosine_similarity(\n",
    "            emb1.reshape(1, -1),\n",
    "            emb2.reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        return float(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868c324a",
   "metadata": {},
   "source": [
    "## Evaluate Input Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06589a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = os.path.splitext(os.path.basename(input_path))[0]\n",
    "output_path = f\"outputs/{base_name}_similarity-scores.xlsx\"\n",
    "\n",
    "def save_df_to_excel_sheet(\n",
    "    output_path: str,\n",
    "    sheet_name: str,\n",
    "    df: pd.DataFrame,\n",
    "    index = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to an Excel file, replacing only the specified sheet.\n",
    "    Creates the file if it does not exist.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    mode = \"a\" if os.path.exists(output_path) else \"w\"\n",
    "\n",
    "    with pd.ExcelWriter(\n",
    "        output_path,\n",
    "        engine=\"openpyxl\",\n",
    "        mode=mode,\n",
    "        if_sheet_exists=\"replace\" if mode == \"a\" else None\n",
    "    ) as writer:\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=index)\n",
    "\n",
    "    print(f\"Saved sheet '{sheet_name}' to: {output_path}\")\n",
    "\n",
    "def safe_text(val):\n",
    "    if pd.isna(val) or val is None:\n",
    "        return \"\"\n",
    "    return str(val).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3fa0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluating {len(df)} rows...\")\n",
    "\n",
    "# Pre-sanitize inputs once\n",
    "facts_list = [safe_text(v) for v in df[col_manual]]\n",
    "generated_list = [safe_text(v) for v in df[col_generated]]\n",
    "\n",
    "for model_name in model_list:\n",
    "    print(f\"\\nUsing model: {model_name}\")\n",
    "    evaluator = Evaluator(model_name)\n",
    "\n",
    "    similarity_scores = []\n",
    "    count = 1\n",
    "\n",
    "    for facts, generated in zip(facts_list, generated_list):\n",
    "        print(f\"evaluating row {count} | \", end=\"\")\n",
    "\n",
    "        score = 0.0\n",
    "        if facts and generated:\n",
    "            score = evaluator.get_semantic_similarity(facts, generated)\n",
    "\n",
    "        similarity_scores.append(score)\n",
    "        print(f\"{model_name}: {score}\")\n",
    "        count += 1\n",
    "\n",
    "    # Column header is the model name\n",
    "    df[model_name] = similarity_scores\n",
    "    save_df_to_excel_sheet(output_path, sheet_name, df)\n",
    "\n",
    "print(f\"Evaluation of sheet '{sheet_name}' at {input_path} done, saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a861292",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_name_aggregate = f\"{sheet_name}-aggregated\"\n",
    "\n",
    "def analyze_similarity_by_model(\n",
    "    df: pd.DataFrame,\n",
    "    model_cols: list[str],\n",
    "    filter_col: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns three DataFrames with aggregate stats per model:\n",
    "    1) all rows\n",
    "    2) rows where filter_col == TRUE\n",
    "    3) rows where filter_col == FALSE\n",
    "    \"\"\"\n",
    "\n",
    "    def _aggregate(sub_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        n = len(sub_df)\n",
    "        return pd.DataFrame({\n",
    "            \"mean\": sub_df.mean(),\n",
    "            \"median\": sub_df.median(),\n",
    "            \"% above 0.60\": (sub_df > 0.60).sum() / n * 100 if n else 0,\n",
    "            \"% above 0.70\": (sub_df > 0.70).sum() / n * 100 if n else 0,\n",
    "            \"% above 0.80\": (sub_df > 0.80).sum() / n * 100 if n else 0,\n",
    "            \"std. dev\": sub_df.std(),\n",
    "        })\n",
    "    \n",
    "    \n",
    "\n",
    "    df_all = _aggregate(df[model_cols])\n",
    "\n",
    "    mask_true = df[filter_col].astype(str).str.strip().str.upper() == \"TRUE\"\n",
    "    mask_false = df[filter_col].astype(str).str.strip().str.upper() == \"FALSE\"\n",
    "\n",
    "    df_true = _aggregate(df[mask_true][model_cols])\n",
    "    df_false = _aggregate(df[mask_false][model_cols])\n",
    "\n",
    "\n",
    "    return df_all, df_true, df_false\n",
    "\n",
    "df_all, df_true, df_false = analyze_similarity_by_model(df, model_list, \"Match\")\n",
    "\n",
    "merged_df = (\n",
    "    df_all.add_prefix(\"ALL__\")\n",
    "    .join(df_true.add_prefix(\"TRUE__\"))\n",
    "    .join(df_false.add_prefix(\"FALSE__\"))\n",
    ")\n",
    "\n",
    "metrics = df_all.columns\n",
    "merged_df = merged_df[\n",
    "    [f\"{p}__{m}\" for m in metrics for p in (\"ALL\", \"TRUE\", \"FALSE\")]\n",
    "]\n",
    "\n",
    "save_df_to_excel_sheet(output_path, sheet_name_aggregate, merged_df, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patunai-ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
