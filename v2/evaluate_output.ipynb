{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "227f7fa2",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dac11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d635a",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_score(df: pd.DataFrame, col_match: str = \"Match\") -> float:\n",
    "    if col_match not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain a 'Match' column.\")\n",
    "    \n",
    "    valid_matches = df[col_match].dropna()\n",
    "    if len(valid_matches) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return valid_matches.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1cd088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text) -> str:\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62154ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"outputs/text_only_1.xlsx\"\n",
    "col_original_premise = \"Premise/Facts\"\n",
    "col_generated_premise = \"Generated Premise\"\n",
    "col_match = \"Match\"\n",
    "\n",
    "df = pd.read_excel(path)\n",
    "df[col_original_premise] = df[col_original_premise].apply(normalize_text)\n",
    "df[col_generated_premise] = df[col_generated_premise].apply(normalize_text)\n",
    "\n",
    "match_score = match_score(df, col_match=col_match)\n",
    "print(f\"Retrieval accuracy: {match_score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d13401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PremiseEvaluator:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    def cosine_similarity_score(self, original: str, generated: str) -> float:\n",
    "        \"\"\"Calculate cosine similarity between TF-IDF vectors\"\"\"\n",
    "        try:\n",
    "            vectors = self.vectorizer.fit_transform([original, generated])\n",
    "            return cosine_similarity(vectors[0:1], vectors[1:2])[0][0]\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def bleu_score(self, original: str, generated: str) -> float:\n",
    "        \"\"\"Calculate BLEU score for text similarity\"\"\"\n",
    "        reference = [word_tokenize(original)]\n",
    "        candidate = word_tokenize(generated)\n",
    "        return sentence_bleu(reference, candidate)\n",
    "    \n",
    "    def length_ratio(self, original: str, generated: str) -> float:\n",
    "        \"\"\"Compare length of generated vs original\"\"\"\n",
    "        len_orig = len(original.split())\n",
    "        len_gen = len(generated.split())\n",
    "        if len_orig == 0:\n",
    "            return 0.0\n",
    "        return len_gen / len_orig\n",
    "\n",
    "evaluator = PremiseEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for idx, row in df.iterrows():\n",
    "    original = row[col_original_premise]\n",
    "    generated = row[col_generated_premise]\n",
    "    \n",
    "    if original and generated:\n",
    "        results.append({\n",
    "            'index': idx,\n",
    "            'cosine_similarity': evaluator.cosine_similarity_score(original, generated),\n",
    "            'bleu_score': evaluator.bleu_score(original, generated),\n",
    "            'length_ratio': evaluator.length_ratio(original, generated),\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3fa0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate aggregate statistics for evaluation metrics\n",
    "aggregate_stats = results_df[['cosine_similarity', 'bleu_score', 'length_ratio']].agg(['mean', 'min', 'max', 'std', 'median'])\n",
    "print(\"Aggregate Statistics for Evaluation Metrics:\")\n",
    "print(aggregate_stats)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Additional summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "print(f\"Total valid results: {len(results_df)}\")\n",
    "print(f\"Average Cosine Similarity: {results_df['cosine_similarity'].mean():.4f}\")\n",
    "print(f\"Average BLEU Score: {results_df['bleu_score'].mean():.4f}\")\n",
    "print(f\"Average Length Ratio: {results_df['length_ratio'].mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patunai-ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
